{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "In this notebook we're going to look at how we can 'mine' concepts from a corpus (collection) of text documents.\n",
    "\n",
    "In the first week of class everyone wrote their own definition of data science.   This week I'm going to show you how to extract 'concepts' from that corpus mathematically.  The techinque we're going to use is called latent semantic analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peymanmohajerian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this only once\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the newsgroup data 'ec.sport.baseball'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['rec.sport.baseball']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [x.lower() for x in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words that I don't want to convert to featurs,becuase they aren't especially useful.  Words like 'a', 'and', and 'the' are good stopwords in english.   I can use a built in list of stopwords from nltk to get me started.  Then, I'll add some custom stopwords that are 'html junk' that I need to clean out of my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title'\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class', 'nntp', \n",
    "               '00', 'would', '00 00', '00 00 00', 'edu', 'com', 'david', 'lafayette', '000 000 151', '000 000 74',\n",
    "               '000 000', '000 000 000', '10', '000 000', '000 000 crunch', '000 000 74', '000 000 067',\n",
    "               '000 000 assuming'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "\n",
    "I'm going to use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from: writingctr@leo.bsuvc.bsu.edu\\nsubject: re: cub fever.\\norganization: ball state university, muncie, in - univ. computing svc's\\nlines: 21\\n\\n\\nin article <kingoz.735285670@camelot>, kingoz@camelot.bradley.edu (orin roth) writes:\\n> \\n>    cub fever is hitting me again. i'm beginning to think they have a \\n>    chance this year. (what the heck am i thinking?)\\n>    sorry. just a moment of incompetence.\\n>    i'll be ok. really. \\n>    orin.\\n>    bradley u.\\n> \\n> --\\n> i'm really a jester in disguise!                                   \\ni hear ya!  then again, we must remember that we are indeed cub fans, and\\nthat the cubs will eventually blow it.  after all, the cubs are the easiest\\nteam in the national league to root for.  no pressure.  you know they will\\nlose eventually.  oh well, i suppose we must have faith.  after all, they\\ndo look pretty good, and they don't even have sandberg back yet.  \\n\\ncubs in '93!!!!!\\n\\ncha\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before!\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset,\n",
    "                                 use_idf=True, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x186595 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 224 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "Input:  X, a matrix where m is the number of documents I have, and n is the number of terms.\n",
    "\n",
    "Process:   I'm going to decompose X into three matricies called U, S, and T.  When we do the decomposition, we have to pick a value k, that's how many concepts we are going to keep.  \n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "U will be a m x k matrix.  The rows will be documents and the columns will be 'concepts'\n",
    "\n",
    "S will be a k x k diagnal matrix.   The elements will be the amount of variation captured from each concept.\n",
    "\n",
    "V will be a n x k (mind the transpose) matrix.   The rows will be terms and the columns will be conepts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 186595)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=17, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=17, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01545924,  0.0019262 ,  0.000441  , ...,  0.00111875,\n",
       "        0.00111875,  0.00111875])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "year\n",
      "team\n",
      "writes\n",
      "game\n",
      "cs\n",
      "article\n",
      "baseball\n",
      "players\n",
      "games\n",
      "one\n",
      " \n",
      "Concept 1:\n",
      "host\n",
      "subject\n",
      "jewish\n",
      "may\n",
      "two\n",
      "roger\n",
      "players\n",
      "jewish baseball\n",
      "games\n",
      "game\n",
      " \n",
      "Concept 2:\n",
      "braves\n",
      "university\n",
      "lines\n",
      "think\n",
      "article\n",
      "hirschbeck\n",
      "time\n",
      "big\n",
      "gant\n",
      "years\n",
      " \n",
      "Concept 3:\n",
      "players\n",
      "runs\n",
      "pitching\n",
      "host\n",
      "posting host\n",
      "one\n",
      "000 000 067\n",
      "something\n",
      "good\n",
      "time\n",
      " \n",
      "Concept 4:\n",
      "one\n",
      "runs\n",
      "games\n",
      "baseball\n",
      "morris\n",
      "also\n",
      "know\n",
      "year\n",
      "make\n",
      "hitter\n",
      " \n",
      "Concept 5:\n",
      "000 000 assuming\n",
      "first\n",
      "say\n",
      "pitcher\n",
      "could\n",
      "writes article\n",
      "subject\n",
      "two\n",
      "year\n",
      "last year\n",
      " \n",
      "Concept 6:\n",
      "good\n",
      "better\n",
      "roger\n",
      "time\n",
      "lot\n",
      "players\n",
      "hitter\n",
      "think\n",
      "000 000 crunch\n",
      "000 000 151\n",
      " \n",
      "Concept 7:\n",
      "team\n",
      "games\n",
      "mets\n",
      "posting host\n",
      "john\n",
      "year\n",
      "sox\n",
      "little\n",
      "play\n",
      "000 000 74\n",
      " \n",
      "Concept 8:\n",
      "game\n",
      "cs\n",
      "article\n",
      "think\n",
      "best\n",
      "say\n",
      "jewish\n",
      "roger\n",
      "ca\n",
      "organization university\n",
      " \n",
      "Concept 9:\n",
      "team\n",
      "university\n",
      "players\n",
      "baseball\n",
      "games\n",
      "subject\n",
      "years\n",
      "least\n",
      "something\n",
      "ted\n",
      " \n",
      "Concept 10:\n",
      "games\n",
      "writes\n",
      "one\n",
      "university\n",
      "really\n",
      "say\n",
      "last year\n",
      "posting host\n",
      "000 000\n",
      "hit\n",
      " \n",
      "Concept 11:\n",
      "year\n",
      "time\n",
      "university\n",
      "last\n",
      "000 000 assuming\n",
      "win\n",
      "also\n",
      "players\n",
      "good\n",
      "maybe\n",
      " \n",
      "Concept 12:\n",
      "good\n",
      "well\n",
      "better\n",
      "two\n",
      "lines\n",
      "university\n",
      "cs\n",
      "time\n",
      "hit\n",
      "take\n",
      " \n",
      "Concept 13:\n",
      "even\n",
      "first\n",
      "get\n",
      "cs\n",
      "posting\n",
      "two\n",
      "morris\n",
      "writes article\n",
      "university\n",
      "make\n",
      " \n",
      "Concept 14:\n",
      "cs\n",
      "one\n",
      "good\n",
      "go\n",
      "first\n",
      "team\n",
      "people\n",
      "writes\n",
      "many\n",
      "article\n",
      " \n",
      "Concept 15:\n",
      "writes\n",
      "baseball\n",
      "article\n",
      "one\n",
      "win\n",
      "well\n",
      "posting host\n",
      "cs\n",
      "time\n",
      "year\n",
      " \n",
      "Concept 16:\n",
      "000 000 crunch\n",
      "lines\n",
      "baseball\n",
      "university\n",
      "team\n",
      "subject\n",
      "writes\n",
      "000 000\n",
      "article\n",
      "000 000 assuming\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.54592373e-02,   1.92619966e-03,   4.40999914e-04, ...,\n",
       "          1.11875033e-03,   1.11875033e-03,   1.11875033e-03],\n",
       "       [  5.06577499e-03,  -5.19664411e-02,  -3.20938720e-02, ...,\n",
       "          1.58752946e-04,   6.16250150e-05,   6.16250150e-05],\n",
       "       [  3.86491324e-03,   5.10748277e-02,   2.81205607e-02, ...,\n",
       "          1.03794495e-03,   9.88211495e-04,   9.88211495e-04],\n",
       "       ..., \n",
       "       [ -5.58862633e-02,   9.87913578e-04,  -1.43179243e-02, ...,\n",
       "         -1.33125616e-04,  -1.57117708e-04,  -1.57117708e-04],\n",
       "       [  4.50331924e-02,   5.36283695e-02,   5.65795199e-03, ...,\n",
       "         -1.48653161e-03,  -1.46668337e-03,  -1.46668337e-03],\n",
       "       [ -5.21057368e-02,   1.20813186e-01,   4.09935571e-02, ...,\n",
       "          1.09470061e-03,   1.07514995e-03,   1.07514995e-03]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
